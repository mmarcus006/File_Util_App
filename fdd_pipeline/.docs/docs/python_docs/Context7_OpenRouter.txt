TITLE: Authenticate OpenRouter API with OpenAI SDK (Python)
DESCRIPTION: This Python snippet demonstrates how to use the OpenAI Python library to make requests to the OpenRouter API. It requires setting the `openai.api_base` to the OpenRouter endpoint and assigning your API key to `openai.api_key`. Optional headers can be passed directly to the `ChatCompletion.create` method.
SOURCE: https://openrouter.ai/docs/api-reference/authentication.mdx#_snippet_2

LANGUAGE: python
CODE:
```
import openai

openai.api_base = "https://openrouter.ai/api/v1"
openai.api_key = "<OPENROUTER_API_KEY>"

response = openai.ChatCompletion.create(
  model="openai/gpt-4o",
  messages=[...],
  headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
)

reply = response.choices[0].message
```

----------------------------------------

TITLE: Initializing and Running LangChain ChatOpenAI with OpenRouter (Python)
DESCRIPTION: Shows a complete Python example using LangChain to interact with OpenRouter. It demonstrates loading environment variables, setting up a prompt template, initializing `ChatOpenAI` with OpenRouter base URL and headers, creating an `LLMChain`, and running a query.
SOURCE: https://openrouter.ai/docs/community/frameworks.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from os import getenv
from dotenv import load_dotenv

load_dotenv()

template = """Question: {question}
Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])

llm = ChatOpenAI(
  openai_api_key=getenv("OPENROUTER_API_KEY"),
  openai_api_base=getenv("OPENROUTER_BASE_URL"),
  model_name="<model_name>",
  model_kwargs={
    "headers": {
      "HTTP-Referer": getenv("YOUR_SITE_URL"),
      "X-Title": getenv("YOUR_SITE_NAME"),
    }
  },
)

llm_chain = LLMChain(prompt=prompt, llm=llm)

question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"

print(llm_chain.run(question))
```

----------------------------------------

TITLE: Streaming Text and Using Tools with Vercel AI SDK and OpenRouter (TypeScript)
DESCRIPTION: Shows how to use the `@openrouter/ai-sdk-provider` with the Vercel AI SDK `streamText` function. It includes examples for streaming a recipe and using a tool (`getCurrentWeather`) with schema validation (`zod`) to handle function calls.
SOURCE: https://openrouter.ai/docs/community/frameworks.mdx#_snippet_7

LANGUAGE: typescript
CODE:
```
import { createOpenRouter } from '@openrouter/ai-sdk-provider';
import { streamText } from 'ai';
import { z } from 'zod';

export const getLasagnaRecipe = async (modelName: string) => {
  const openrouter = createOpenRouter({
    apiKey: '${API_KEY_REF}',
  });

  const response = streamText({
    model: openrouter(modelName),
    prompt: 'Write a vegetarian lasagna recipe for 4 people.',
  });

  await response.consumeStream();
  return response.text;
};

export const getWeather = async (modelName: string) => {
  const openrouter = createOpenRouter({
    apiKey: '${API_KEY_REF}',
  });

  const response = streamText({
    model: openrouter(modelName),
    prompt: 'What is the weather in San Francisco, CA in Fahrenheit?',
    tools: {
      getCurrentWeather: {
        description: 'Get the current weather in a given location',
        parameters: z.object({
          location: z
            .string()
            .describe('The city and state, e.g. San Francisco, CA'),
          unit: z.enum(['celsius', 'fahrenheit']).optional(),
        }),
        execute: async ({ location, unit = 'celsius' }) => {
          // Mock response for the weather
          const weatherData = {
            'Boston, MA': {
              celsius: '15째C',
              fahrenheit: '59째F',
            },
            'San Francisco, CA': {
              celsius: '18째C',
              fahrenheit: '64째F',
            },
          };

          const weather = weatherData[location];
          if (!weather) {
            return `Weather data for ${location} is not available.`;
          }

          return `The current weather in ${location} is ${weather[unit]}.`;
        },
      },
    },
  });

  await response.consumeStream();
  return response.text;
};
```

----------------------------------------

TITLE: Sending Chat Completion Request with Python Requests
DESCRIPTION: Shows how to make a POST request to the OpenRouter chat completions API using the Python `requests` library, setting the necessary headers and sending the request payload as JSON.
SOURCE: https://openrouter.ai/docs/api-reference/chat-completion.mdx#_snippet_2

LANGUAGE: python
CODE:
```
import requests

url = "https://openrouter.ai/api/v1/chat/completions"

payload = { "model": "openai/gpt-3.5-turbo" }
headers = {
    "Authorization": "Bearer <token>",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

----------------------------------------

TITLE: Sending Chat Completion Request with JavaScript Fetch
DESCRIPTION: Illustrates how to use the JavaScript `fetch` API to send a POST request to the OpenRouter chat completions endpoint, configuring headers and the JSON request body, and handling the response asynchronously.
SOURCE: https://openrouter.ai/docs/api-reference/chat-completion.mdx#_snippet_3

LANGUAGE: javascript
CODE:
```
const url = 'https://openrouter.ai/api/v1/chat/completions';
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"model":"openai/gpt-3.5-turbo"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

----------------------------------------

TITLE: Calling OpenRouter API directly
DESCRIPTION: Demonstrates how to interact with the OpenRouter API by making direct HTTP requests to the chat completions endpoint. It shows how to include the API key in the Authorization header and add optional headers for site ranking. Examples are provided using Python (requests), TypeScript (fetch), and Shell (curl).
SOURCE: https://openrouter.ai/docs/quickstart.mdx#_snippet_1

LANGUAGE: python
CODE:
```
import requests
import json

response = requests.post(
  url="https://openrouter.ai/api/v1/chat/completions",
  headers={
    "Authorization": "Bearer <OPENROUTER_API_KEY>",
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  data=json.dumps({
    "model": "openai/gpt-4o", # Optional
    "messages": [
      {
        "role": "user",
        "content": "What is the meaning of life?"
      }
    ]
  })
)
```

LANGUAGE: typescript
CODE:
```
fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  }),
});
```

LANGUAGE: shell
CODE:
```
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -d '{
  "model": "openai/gpt-4o",
  "messages": [
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
}'
```

----------------------------------------

TITLE: Calling OpenRouter API using OpenAI SDK
DESCRIPTION: Demonstrates how to integrate with the OpenRouter API using the official OpenAI SDKs. It covers setting the base URL, providing the API key, and including optional headers for site ranking. Examples are provided for Python and TypeScript, showing how to make a chat completion request. Requires the respective OpenAI SDK library.
SOURCE: https://openrouter.ai/docs/quickstart.mdx#_snippet_0

LANGUAGE: python
CODE:
```
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="<OPENROUTER_API_KEY>",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  model="openai/gpt-4o",
  messages=[
    {
      "role": "user",
      "content": "What is the meaning of life?"
    }
  ]
)

print(completion.choices[0].message.content)
```

LANGUAGE: typescript
CODE:
```
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
  defaultHeaders: {
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
  },
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  });

  console.log(completion.choices[0].message);
}

main();
```

----------------------------------------

TITLE: Making a Chat Completion Request with Headers (TypeScript)
DESCRIPTION: This snippet demonstrates how to make a basic chat completion request to the OpenRouter API using fetch in TypeScript. It shows how to include the required Authorization header and optional HTTP-Referer and X-Title headers for identifying your application on openrouter.ai. The request body specifies the model and the initial user message.
SOURCE: https://openrouter.ai/docs/api-reference/overview.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?'
      }
    ]
  })
});
```

----------------------------------------

TITLE: Implementing Agentic Loop with OpenRouter Python
DESCRIPTION: This Python snippet defines functions to call an LLM via the OpenRouter API, process tool calls returned by the LLM, and execute the corresponding local tool function. It then uses a `while` loop to repeatedly call the LLM and process tool responses until the LLM provides a final non-tool response.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_6

LANGUAGE: python
CODE:
```
def call_llm(msgs):
    resp = openai_client.chat.completions.create(
        model={{MODEL}},
        tools=tools,
        messages=msgs
    )
    msgs.append(resp.choices[0].message.dict())
    return resp

def get_tool_response(response):
    tool_call = response.choices[0].message.tool_calls[0]
    tool_name = tool_call.function.name
    tool_args = json.loads(tool_call.function.arguments)

    # Look up the correct tool locally, and call it with the provided arguments
    # Other tools can be added without changing the agentic loop
    tool_result = TOOL_MAPPING[tool_name](**tool_args)

    return {
        "role": "tool",
        "tool_call_id": tool_call.id,
        "name": tool_name,
        "content": tool_result,
    }

while True:
    resp = call_llm(_messages)

    if resp.choices[0].message.tool_calls is not None:
        messages.append(get_tool_response(resp))
    else:
        break

print(messages[-1]['content'])
```

----------------------------------------

TITLE: Initializing OpenAI SDK with OpenRouter (Python)
DESCRIPTION: Shows how to initialize the OpenAI SDK in Python to connect to OpenRouter, setting the base URL and API key from environment variables. It includes examples of using `extra_headers` for site information and `extra_body` for OpenRouter-specific arguments (commented out).
SOURCE: https://openrouter.ai/docs/community/frameworks.mdx#_snippet_1

LANGUAGE: Python
CODE:
```
from openai import OpenAI
from os import getenv

# gets API Key from environment variable OPENAI_API_KEY
client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=getenv("OPENROUTER_API_KEY"),
)

completion = client.chat.completions.create(
  model="${Model.GPT_4_Omni}",
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  # pass extra_body to access OpenRouter-only arguments.
  # extra_body={
    # "models": [
    #   "${Model.GPT_4_Omni}",
    #   "${Model.Mixtral_8x_22B_Instruct}"
    # ]
  # },
  messages=[
    {
      "role": "user",
      "content": "Say this is a test",
    },
  ],
)
print(completion.choices[0].message.content)
```

----------------------------------------

TITLE: Implementing Agentic Loop with OpenRouter TypeScript
DESCRIPTION: This TypeScript snippet defines asynchronous functions to call an LLM via the OpenRouter API using `fetch`, process tool calls from the LLM's response, and execute the corresponding local tool function. It uses an `async while` loop to manage the conversation flow, calling the LLM and processing tool results until a non-tool response is received.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_7

LANGUAGE: typescript
CODE:
```
async function callLLM(messages: Message[]): Promise<Message> {
  const response = await fetch(
    'https://openrouter.ai/api/v1/chat/completions',
    {
      method: 'POST',
      headers: {
        Authorization: `Bearer {{API_KEY_REF}}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: '{{MODEL}}',
        tools,
        messages,
      }),
    },
  );

  const data = await response.json();
  messages.push(data.choices[0].message);
  return data;
}

async function getToolResponse(response: Message): Promise<Message> {
  const toolCall = response.toolCalls[0];
  const toolName = toolCall.function.name;
  const toolArgs = JSON.parse(toolCall.function.arguments);

  // Look up the correct tool locally, and call it with the provided arguments
  // Other tools can be added without changing the agentic loop
  const toolResult = await TOOL_MAPPING[toolName](toolArgs);

  return {
    role: 'tool',
    toolCallId: toolCall.id,
    name: toolName,
    content: toolResult,
  };
}

while (true) {
  const response = await callLLM(messages);

  if (response.toolCalls) {
    messages.push(await getToolResponse(response));
  } else {
    break;
  }
}

console.log(messages[messages.length - 1].content);
```

----------------------------------------

TITLE: Streaming Chat Completion with Reasoning Tokens using OpenRouter API in TypeScript
DESCRIPTION: Illustrates how to perform a streaming chat completion request to the OpenRouter API using the TypeScript `openai` library. The example configures a specific token limit for reasoning and iterates through the streamed response to log reasoning and content parts.
SOURCE: https://openrouter.ai/docs/use-cases/reasoning-tokens.mdx#_snippet_9

LANGUAGE: TypeScript
CODE:
```
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey,
});

async function chatCompletionWithReasoning(messages) {
  const response = await openai.chat.completions.create({
    model: '{{MODEL}}',
    messages,
    maxTokens: 10000,
    reasoning: {
      maxTokens: 8000, // Directly specify reasoning token budget
    },
    stream: true,
  });

  return response;
}

(async () => {
  for await (const chunk of chatCompletionWithReasoning([
    { role: 'user', content: "What's bigger, 9.9 or 9.11?" },
  ])) {
    if (chunk.choices[0].delta.reasoning) {
      console.log(`REASONING: ${chunk.choices[0].delta.reasoning}`);
    } else if (chunk.choices[0].delta.content) {
      console.log(`CONTENT: ${chunk.choices[0].delta.content}`);
    }
  }
})();
```

----------------------------------------

TITLE: Initializing OpenAI SDK with OpenRouter (TypeScript)
DESCRIPTION: Demonstrates how to initialize the OpenAI SDK in TypeScript to use OpenRouter as the base URL, including setting the API key and default headers. It then shows how to create a chat completion.
SOURCE: https://openrouter.ai/docs/community/frameworks.mdx#_snippet_0

LANGUAGE: TypeScript
CODE:
```
import OpenAI from "openai"

const openai = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: "${API_KEY_REF}",
  defaultHeaders: {
    ${getHeaderLines().join('\n        ')}
  },
})

async function main() {
  const completion = await openai.chat.completions.create({
    model: "${Model.GPT_4_Omni}",
    messages: [
      { role: "user", content: "Say this is a test" }
    ],
  })

  console.log(completion.choices[0].message)
}
main();
```

----------------------------------------

TITLE: Sending Completions Request with JavaScript Fetch
DESCRIPTION: Example using the JavaScript Fetch API to send an asynchronous POST request to the OpenRouter completions endpoint, including headers and a stringified JSON body.
SOURCE: https://openrouter.ai/docs/api-reference/completion.mdx#_snippet_3

LANGUAGE: javascript
CODE:
```
const url = 'https://openrouter.ai/api/v1/completions';
const options = {
  method: 'POST',
  headers: {Authorization: 'Bearer <token>', 'Content-Type': 'application/json'},
  body: '{"model":"model","prompt":"prompt"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

----------------------------------------

TITLE: Making Initial Chat Completion Call with TypeScript
DESCRIPTION: This TypeScript snippet shows how to make the initial API call to OpenRouter's chat completions endpoint using the fetch API. It sets the required headers, including authorization with the API key and content type, and includes the model and initial messages in the request body.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: `Bearer {{API_KEY_REF}}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: '{{MODEL}}',
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      {
        role: 'user',
        content: 'What are the titles of some James Joyce books?',
      },
    ],
  }),
});
```

----------------------------------------

TITLE: Implement Chain-of-Thought Reasoning with OpenRouter API (Python)
DESCRIPTION: This Python snippet demonstrates a chain-of-thought workflow using the OpenRouter API. It first queries a capable model (`deepseek/deepseek-r1`) to generate reasoning for a question, then injects this reasoning into a subsequent query to another model (`openai/gpt-4o-mini`) to improve its final answer. It requires the `requests` and `json` libraries.
SOURCE: https://openrouter.ai/docs/use-cases/reasoning-tokens.mdx#_snippet_6

LANGUAGE: Python
CODE:
```
import requests
import json

question = "Which is bigger: 9.11 or 9.9?"

url = "https://openrouter.ai/api/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {{API_KEY_REF}}",
    "Content-Type": "application/json"
}

def do_req(model, content, reasoning_config=None):
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": content}
        ],
        "stop": "</think>"
    }

    return requests.post(url, headers=headers, data=json.dumps(payload))

# Get reasoning from a capable model
content = f"{question} Please think this through, but don't output an answer"
reasoning_response = do_req("deepseek/deepseek-r1", content)
reasoning = reasoning_response.json()['choices'][0]['message']['reasoning']

# Let's test! Here's the naive response:
simple_response = do_req("openai/gpt-4o-mini", question)
print(simple_response.json()['choices'][0]['message']['content'])

# Here's the response with the reasoning token injected:
content = f"{question}. Here is some context to help you: {reasoning}"
smart_response = do_req("openai/gpt-4o-mini", content)
print(smart_response.json()['choices'][0]['message']['content'])
```

----------------------------------------

TITLE: Authenticate OpenRouter API with OpenAI SDK (TypeScript)
DESCRIPTION: This example shows how to configure and use the OpenAI TypeScript SDK to interact with the OpenRouter API. It involves setting the `baseURL` to the OpenRouter endpoint and providing the API key during client initialization. Optional default headers can also be configured.
SOURCE: https://openrouter.ai/docs/api-reference/authentication.mdx#_snippet_1

LANGUAGE: typescript
CODE:
```
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: '<OPENROUTER_API_KEY>',
  defaultHeaders: {
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
  },
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: 'openai/gpt-4o',
    messages: [{ role: 'user', content: 'Say this is a test' }],
  });

  console.log(completion.choices[0].message);
}

main();
```

----------------------------------------

TITLE: Sending Completions Request with Python Requests
DESCRIPTION: Example using the Python requests library to send a POST request to the OpenRouter completions endpoint with the necessary headers and JSON payload.
SOURCE: https://openrouter.ai/docs/api-reference/completion.mdx#_snippet_2

LANGUAGE: python
CODE:
```
import requests

url = "https://openrouter.ai/api/v1/completions"

payload = {
    "model": "model",
    "prompt": "prompt"
}
headers = {
    "Authorization": "Bearer <token>",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

----------------------------------------

TITLE: Implement Chain-of-Thought Reasoning with OpenRouter API (TypeScript)
DESCRIPTION: This TypeScript snippet shows how to perform a chain-of-thought reasoning workflow using the OpenRouter API via the `openai` library. It fetches reasoning from one model (`deepseek/deepseek-r1`) and uses it as context for a query to another model (`openai/gpt-4o-mini`) to enhance the response quality. It requires the `openai` library configured with the OpenRouter base URL.
SOURCE: https://openrouter.ai/docs/use-cases/reasoning-tokens.mdx#_snippet_7

LANGUAGE: TypeScript
CODE:
```
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey,
});

async function doReq(model, content, reasoningConfig) {
  const payload = {
    model,
    messages: [{ role: 'user', content }],
    stop: '</think>',
    ...reasoningConfig,
  };

  return openai.chat.completions.create(payload);
}

async function getResponseWithReasoning() {
  const question = 'Which is bigger: 9.11 or 9.9?';
  const reasoningResponse = await doReq(
    'deepseek/deepseek-r1',
    `${question} Please think this through, but don't output an answer`,
  );
  const reasoning = reasoningResponse.choices[0].message.reasoning;

  // Let's test! Here's the naive response:
  const simpleResponse = await doReq('openai/gpt-4o-mini', question);
  console.log(simpleResponse.choices[0].message.content);

  // Here's the response with the reasoning token injected:
  const content = `${question}. Here is some context to help you: ${reasoning}`;
  const smartResponse = await doReq('openai/gpt-4o-mini', content);
  console.log(smartResponse.choices[0].message.content);
}

getResponseWithReasoning();
```

----------------------------------------

TITLE: Requesting Structured Output with JSON Schema (TypeScript)
DESCRIPTION: This code snippet demonstrates how to include the 'response_format' parameter in an OpenRouter API request to enforce a specific JSON schema for the model's response. It shows the structure for defining the schema, including properties, types, descriptions, required fields, and strict mode.
SOURCE: https://openrouter.ai/docs/features/structured-outputs.mdx#_snippet_0

LANGUAGE: json
CODE:
```
{
  "messages": [
    { "role": "user", "content": "What's the weather like in London?" }
  ],
  "response_format": {
    "type": "json_schema",
    "json_schema": {
      "name": "weather",
      "strict": true,
      "schema": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "City or location name"
          },
          "temperature": {
            "type": "number",
            "description": "Temperature in Celsius"
          },
          "conditions": {
            "type": "string",
            "description": "Weather conditions description"
          }
        },
        "required": ["location", "temperature", "conditions"],
        "additionalProperties": false
      }
    }
  }
}
```

----------------------------------------

TITLE: Initializing OpenRouter Client with Python
DESCRIPTION: This snippet demonstrates the basic setup for using OpenRouter with Python. It imports necessary libraries, sets the API key and model, initializes the OpenAI client pointing to OpenRouter's base URL, and defines the initial system and user messages for a chat conversation.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_0

LANGUAGE: python
CODE:
```
import json, requests
from openai import OpenAI

OPENROUTER_API_KEY = f"{{API_KEY_REF}}"

# You can use any model that supports tool calling
MODEL = "{{MODEL}}"

openai_client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=OPENROUTER_API_KEY,
)

task = "What are the titles of some James Joyce books?"

messages = [
  {
    "role": "system",
    "content": "You are a helpful assistant."
  },
  {
    "role": "user",
    "content": task,
  }
]
```

----------------------------------------

TITLE: Authenticate OpenRouter API with Bearer Token (TypeScript)
DESCRIPTION: This snippet demonstrates how to make a direct HTTP POST request to the OpenRouter chat completions endpoint using the `fetch` API in TypeScript. It requires setting the `Authorization` header with a Bearer token containing your OpenRouter API key. Optional headers like `HTTP-Referer` and `X-Title` can be included for site ranking.
SOURCE: https://openrouter.ai/docs/api-reference/authentication.mdx#_snippet_0

LANGUAGE: typescript
CODE:
```
fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <OPENROUTER_API_KEY>',
    'HTTP-Referer': '<YOUR_SITE_URL>', // Optional. Site URL for rankings on openrouter.ai.
    'X-Title': '<YOUR_SITE_NAME>', // Optional. Site title for rankings on openrouter.ai.
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'What is the meaning of life?',
      },
    ],
  }),
});
```

----------------------------------------

TITLE: OpenRouter Chat Completions API Response Format (JSON)
DESCRIPTION: This JSON snippet illustrates the standard structure returned by the OpenRouter chat completions API. It includes details such as the unique response ID, provider, model used, object type, creation timestamp, the generated message content from the assistant, and token usage statistics for the request.
SOURCE: https://openrouter.ai/docs/features/images-and-pdfs.mdx#_snippet_6

LANGUAGE: JSON
CODE:
```
{
  "id": "gen-1234567890",
  "provider": "DeepInfra",
  "model": "google/gemma-3-27b-it",
  "object": "chat.completion",
  "created": 1234567890,
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The document discusses..."
      }
    }
  ],
  "usage": {
    "prompt_tokens": 1000,
    "completion_tokens": 100,
    "total_tokens": 1100
  }
}
```

----------------------------------------

TITLE: Make POST Request with requests in Python
DESCRIPTION: This Python snippet uses the popular 'requests' library to make a POST request. It defines the URL, the JSON payload as a dictionary, and the headers. The `requests.post` function handles sending the request and automatically serializes the `json` payload.
SOURCE: https://openrouter.ai/docs/api-reference/authentication/exchange-authorization-code-for-api-key.mdx#_snippet_11

LANGUAGE: python
CODE:
```
import requests

url = "https://openrouter.ai/api/v1/auth/keys"

payload = { "code": "string" }
headers = {"Content-Type": "application/json"}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

----------------------------------------

TITLE: Making Initial OpenRouter API Call with Tools (Python/TypeScript)
DESCRIPTION: This snippet demonstrates how to make the first API call to the OpenRouter chat completions endpoint. It sends the initial messages and the list of available tools to the specified model. The response is expected to contain tool call requests from the LLM.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_3

LANGUAGE: python
CODE:
```
request_1 = {
    "model": {{MODEL}},
    "tools": tools,
    "messages": messages
}

response_1 = openai_client.chat.completions.create(**request_1).message
```

LANGUAGE: typescript
CODE:
```
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: `Bearer {{API_KEY_REF}}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: '{{MODEL}}',
    tools,
    messages,
  }),
});
```

----------------------------------------

TITLE: Use API Key for Request (TypeScript)
DESCRIPTION: TypeScript code snippet demonstrating how to use the obtained user-controlled API key to make a chat completions request to the OpenRouter API. Include the API key in the `Authorization` header as a Bearer token.
SOURCE: https://openrouter.ai/docs/use-cases/oauth-pkce.mdx#_snippet_6

LANGUAGE: typescript
CODE:
```
fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer <API_KEY>',
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [
      {
        role: 'user',
        content: 'Hello!',
      },
    ],
  }),
});
```

----------------------------------------

TITLE: Authenticate OpenRouter API with cURL (Shell)
DESCRIPTION: This shell command uses `curl` to make a direct HTTP POST request to the OpenRouter chat completions endpoint. It sets the `Content-Type` header to `application/json` and the `Authorization` header with a Bearer token containing the API key, typically read from an environment variable.
SOURCE: https://openrouter.ai/docs/api-reference/authentication.mdx#_snippet_3

LANGUAGE: shell
CODE:
```
curl https://openrouter.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENROUTER_API_KEY" \
  -d '{
  "model": "openai/gpt-4o",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
}'
```

----------------------------------------

TITLE: Processing LLM Tool Calls and Appending Results (Python/TypeScript)
DESCRIPTION: This code processes the tool calls received in the LLM's response. It iterates through the requested tool calls, executes the corresponding local function using a mapping, and appends the tool's output back to the messages array with the 'tool' role. This prepares the conversation history for the next API call.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_4

LANGUAGE: python
CODE:
```
# Append the response to the messages array so the LLM has the full context
# It's easy to forget this step!
messages.append(response_1)

# Now we process the requested tool calls, and use our book lookup tool
for tool_call in response_1.tool_calls:
    '''
    In this case we only provided one tool, so we know what function to call.
    When providing multiple tools, you can inspect `tool_call.function.name`
    to figure out what function you need to call locally.
    '''
    tool_name = tool_call.function.name
    tool_args = json.loads(tool_call.function.arguments)
    tool_response = TOOL_MAPPING[tool_name](**tool_args)
    messages.append({
      "role": "tool",
      "tool_call_id": tool_call.id,
      "name": tool_name,
      "content": json.dumps(tool_response),
    })
```

LANGUAGE: typescript
CODE:
```
// Append the response to the messages array so the LLM has the full context
// It's easy to forget this step!
messages.push(response);

// Now we process the requested tool calls, and use our book lookup tool
for (const toolCall of response.toolCalls) {
  const toolName = toolCall.function.name;
  const toolArgs = JSON.parse(toolCall.function.arguments);
  const toolResponse = await TOOL_MAPPING[toolName](toolArgs);
  messages.push({
    role: 'tool',
    toolCallId: toolCall.id,
    name: toolName,
    content: JSON.stringify(toolResponse),
  });
}
```

----------------------------------------

TITLE: Implement and Specify Project Gutenberg Search Tool
DESCRIPTION: Implements a function to search the Gutendex API for books based on search terms and defines the corresponding JSON specification for LLM function calling. The function takes an array of search terms and returns a simplified list of book results (id, title, authors). The specification details the tool's name, description, and the structure of its `search_terms` parameter.
SOURCE: https://openrouter.ai/docs/features/tool-calling.mdx#_snippet_2

LANGUAGE: python
CODE:
```
def search_gutenberg_books(search_terms):
    search_query = " ".join(search_terms)
    url = "https://gutendex.com/books"
    response = requests.get(url, params={"search": search_query})

    simplified_results = []
    for book in response.json().get("results", []):
        simplified_results.append({
            "id": book.get("id"),
            "title": book.get("title"),
            "authors": book.get("authors")
        })

    return simplified_results

tools = [
  {
    "type": "function",
    "function": {
      "name": "search_gutenberg_books",
      "description": "Search for books in the Project Gutenberg library based on specified search terms",
      "parameters": {
        "type": "object",
        "properties": {
          "search_terms": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of search terms to find books in the Gutenberg library (e.g. ['dickens', 'great'] to search for books by Dickens with 'great' in the title)"
          }
        },
        "required": ["search_terms"]
      }
    }
  }
]

TOOL_MAPPING = {
    "search_gutenberg_books": search_gutenberg_books
}
```

LANGUAGE: typescript
CODE:
```
async function searchGutenbergBooks(searchTerms: string[]): Promise<Book[]> {
  const searchQuery = searchTerms.join(' ');
  const url = 'https://gutendex.com/books';
  const response = await fetch(`${url}?search=${searchQuery}`);
  const data = await response.json();

  return data.results.map((book: any) => ({
    id: book.id,
    title: book.title,
    authors: book.authors,
  }));
}

const tools = [
  {
    type: 'function',
    function: {
      name: 'search_gutenberg_books',
      description:
        'Search for books in the Project Gutenberg library based on specified search terms',
      parameters: {
        type: 'object',
        properties: {
          search_terms: {
            type: 'array',
            items: {
              type: 'string',
            },
            description:
              "List of search terms to find books in the Gutenberg library (e.g. ['dickens', 'great'] to search for books by Dickens with 'great' in the title)",
          },
        },
        required: ['search_terms'],
      },
    },
  },
];

const TOOL_MAPPING = {
  searchGutenbergBooks,
};
```

----------------------------------------

TITLE: Streaming Chat Completion with Reasoning Tokens using OpenRouter API in Python
DESCRIPTION: Demonstrates how to make a streaming chat completion request to the OpenRouter API using the Python `openai` library. It shows how to specify a token budget for reasoning and process the streamed response, printing both reasoning and content chunks as they arrive.
SOURCE: https://openrouter.ai/docs/use-cases/reasoning-tokens.mdx#_snippet_8

LANGUAGE: Python
CODE:
```
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="{{API_KEY_REF}}",
)

def chat_completion_with_reasoning(messages):
    response = client.chat.completions.create(
        model="{{MODEL}}",
        messages=messages,
        max_tokens=10000,
        reasoning={
            "max_tokens": 8000  # Directly specify reasoning token budget
        },
        stream=True
    )
    return response

for chunk in chat_completion_with_reasoning([
    {"role": "user", "content": "What's bigger, 9.9 or 9.11?"}
]):
    if hasattr(chunk.choices[0].delta, 'reasoning') and chunk.choices[0].delta.reasoning:
        print(f"REASONING: {chunk.choices[0].delta.reasoning}")
    elif chunk.choices[0].delta.content:
        print(f"CONTENT: {chunk.choices[0].delta.content}")
```

----------------------------------------

TITLE: Making a Structured Output Request with Fetch API
DESCRIPTION: Demonstrates how to make a POST request to the OpenRouter chat completions endpoint using the Fetch API in TypeScript, including setting headers, providing the model and messages, and defining a JSON Schema for the response format.
SOURCE: https://openrouter.ai/docs/features/structured-outputs.mdx#_snippet_2

LANGUAGE: typescript
CODE:
```
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer {{API_KEY_REF}}',
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: '{{MODEL}}',
    messages: [
      { role: 'user', content: 'What is the weather like in London?' },
    ],
    response_format: {
      type: 'json_schema',
      json_schema: {
        name: 'weather',
        strict: true,
        schema: {
          type: 'object',
          properties: {
            location: {
              type: 'string',
              description: 'City or location name',
            },
            temperature: {
              type: 'number',
              description: 'Temperature in Celsius',
            },
            conditions: {
              type: 'string',
              description: 'Weather conditions description',
            },
          },
          required: ['location', 'temperature', 'conditions'],
          additionalProperties: false,
        },
      },
    },
  }),
});

const data = await response.json();
const weatherInfo = data.choices[0].message.content;
```

----------------------------------------

TITLE: Fetching OpenRouter API Key with Python Requests
DESCRIPTION: This Python snippet utilizes the `requests` library to make a GET request to the OpenRouter API key endpoint. It sets the Authorization header with the bearer token and prints the JSON response.
SOURCE: https://openrouter.ai/docs/api-reference/api-keys/get-current-api-key.mdx#_snippet_2

LANGUAGE: python
CODE:
```
import requests

url = "https://openrouter.ai/api/v1/key"

headers = {"Authorization": "Bearer <token>"}

response = requests.get(url, headers=headers)

print(response.json())
```

----------------------------------------

TITLE: Making a Structured Output Request with Python Requests
DESCRIPTION: Shows how to make a POST request to the OpenRouter chat completions endpoint using the `requests` library in Python, including setting headers, providing the model and messages, and defining a JSON Schema for the response format.
SOURCE: https://openrouter.ai/docs/features/structured-outputs.mdx#_snippet_3

LANGUAGE: python
CODE:
```
import requests
import json

response = requests.post(
  "https://openrouter.ai/api/v1/chat/completions",
  headers={
    "Authorization": f"Bearer {{API_KEY_REF}}",
    "Content-Type": "application/json",
  },

  json={
    "model": "{{MODEL}}",
    "messages": [
      {"role": "user", "content": "What is the weather like in London?"},
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "weather",
        "strict": True,
        "schema": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "City or location name",
            },
            "temperature": {
              "type": "number",
              "description": "Temperature in Celsius",
            },
            "conditions": {
              "type": "string",
              "description": "Weather conditions description",
            },
          },
          "required": ["location", "temperature", "conditions"],
          "additionalProperties": False,
        },
      },
    },
  },
)

data = response.json()
weather_info = data["choices"][0]["message"]["content"]
```

----------------------------------------

TITLE: Example Structured Output Response (JSON)
DESCRIPTION: This snippet shows an example of a model response that conforms to the JSON schema defined in the request. It illustrates the expected output format when structured outputs are successfully applied.
SOURCE: https://openrouter.ai/docs/features/structured-outputs.mdx#_snippet_1

LANGUAGE: json
CODE:
```
{
  "location": "London",
  "temperature": 18,
  "conditions": "Partly cloudy with light drizzle"
}
```